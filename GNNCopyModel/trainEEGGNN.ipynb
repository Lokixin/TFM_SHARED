{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load dataset\n",
    "from joblib import load\n",
    "\n",
    "features_path = \"C:/Users/lokix/Desktop/figshare_upload/psd_features_data_X\"\n",
    "labels_path = \"C:/Users/lokix/Desktop/figshare_upload/labels_y\"\n",
    "master_path = \"C:/Users/lokix/Desktop/figshare_upload/master_metadata_index.csv\"\n",
    "\n",
    "features_path = \"C:/Users/lokix/OneDrive - Universitat Politècnica de Catalunya/Escritorio/figshare_upload/psd_features_data_X\"\n",
    "labels_path =\"C:/Users/lokix/OneDrive - Universitat Politècnica de Catalunya/Escritorio/figshare_upload/labels_y\"\n",
    "master_path =\"C:/Users/lokix/OneDrive - Universitat Politècnica de Catalunya/Escritorio/figshare_upload/master_metadata_index.csv\"\n",
    "\n",
    "X = load(features_path, mmap_mode=\"r\")\n",
    "y = load(labels_path, mmap_mode=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10.375448936366148, 1.1066615590130442)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute weights for CrossEntropyLoss\n",
    "_labels = list(y)\n",
    "healthy_count = _labels.count(\"healthy\")\n",
    "disease_count = len(_labels) - healthy_count\n",
    "\n",
    "healthy_weight = 1 / (healthy_count / len(_labels))\n",
    "diseased_weight = 1 / (disease_count / len(_labels))\n",
    "healthy_weight, diseased_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from EEGDataset import EEGDataset, EEGGraphDataset\n",
    "from GNNModel import EEGGNN\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lokix\\AppData\\Local\\Temp\\ipykernel_11388\\1487538665.py:4: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  MASTER_DATASET_INDEX = pd.read_csv(master_path)\n"
     ]
    }
   ],
   "source": [
    "SFREQ = 250.0\n",
    "SEED = 42\n",
    "\n",
    "MASTER_DATASET_INDEX = pd.read_csv(master_path)\n",
    "subjects = MASTER_DATASET_INDEX[\"patient_ID\"].astype(\"str\").unique()\n",
    "train_subjects, test_subjects = train_test_split(subjects, test_size=0.30, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, len(y)\n",
    "df = pd.DataFrame(X)\n",
    "df[\"labels\"] = y\n",
    "healthy_df = df[df[\"labels\"] == \"healthy\"]\n",
    "diseased_df = df[df[\"labels\"] == \"diseased\"].sample(n = healthy_count, random_state=1)\n",
    "final_df = pd.concat([healthy_df, diseased_df])\n",
    "\n",
    "new_labels = final_df[\"labels\"]\n",
    "new_labels = new_labels.to_numpy()\n",
    "\n",
    "new_features = final_df.loc[:, final_df.columns != \"labels\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "heldout_train_indices = MASTER_DATASET_INDEX.index[MASTER_DATASET_INDEX[\"patient_ID\"].astype(\"str\").isin(train_subjects)].tolist()\n",
    "heldout_test_indices = MASTER_DATASET_INDEX.index[MASTER_DATASET_INDEX[\"patient_ID\"].astype(\"str\").isin(test_subjects)].tolist()\n",
    "\n",
    "train_dataset = EEGGraphDataset(X=X,\n",
    "                     y=y,\n",
    "                     indices=heldout_train_indices,\n",
    "                     loader_type=\"heldout_test\",\n",
    "                     sfreq=SFREQ,\n",
    "                     transform=Compose([ToTensor()]))\n",
    "\n",
    "test_dataset = EEGGraphDataset(X=X,\n",
    "                     y=y,\n",
    "                     indices=heldout_test_indices,\n",
    "                     loader_type=\"heldout_test\",\n",
    "                     sfreq=SFREQ,\n",
    "                     transform=Compose([ToTensor()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "NUM_WORKERS = 0\n",
    "PIN_MEMORY = True\n",
    "\n",
    "train_batches = DataLoader(dataset=train_dataset,\n",
    "                           batch_size=BATCH_SIZE,\n",
    "                           shuffle=True,\n",
    "                           num_workers=NUM_WORKERS,\n",
    "                           pin_memory=PIN_MEMORY)\n",
    "\n",
    "test_batches = DataLoader(dataset=test_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=True,\n",
    "                          num_workers=NUM_WORKERS,\n",
    "                          pin_memory=PIN_MEMORY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 7210678410.79906\n",
      "Loss: 72540671.976786\n",
      "Loss: 24760372.112144817\n",
      "Loss: 13802988.310360303\n",
      "Loss: 10724587.744135011\n",
      "Loss: 9212692.767616805\n",
      "Loss: 9379399.971893933\n",
      "Loss: 6648821.961562646\n",
      "Loss: 7232993.854711801\n",
      "Loss: 6749539.04760721\n",
      "Loss: 6485334.097803163\n",
      "Loss: 6301568.215493801\n",
      "Loss: 5862908.402593645\n",
      "Loss: 5270898.477562905\n",
      "Loss: 5513781.365551241\n",
      "Loss: 4914882.937575199\n",
      "Loss: 5709992.226126248\n",
      "Loss: 4090622.5914053814\n",
      "Loss: 3749294.9402135867\n",
      "Loss: 4089294.3337857444\n",
      "Loss: 3917145.0630449355\n",
      "Loss: 3447165.821791072\n",
      "Loss: 3122092.401383856\n",
      "Loss: 2852053.106848383\n",
      "Loss: 2686305.6796188923\n",
      "Loss: 2635753.7787665934\n",
      "Loss: 2134596.744633278\n",
      "Loss: 1879496.157961756\n",
      "Loss: 1868284.0622059477\n",
      "Loss: 1600888.5697240112\n",
      "Loss: 1355054.6946411752\n",
      "Loss: 1320857.805134519\n",
      "Loss: 1177707.1410233043\n",
      "Loss: 1114571.0154713735\n",
      "Loss: 1262170.5811538845\n",
      "Loss: 1179767.2708380914\n",
      "Loss: 1037922.6858531687\n",
      "Loss: 995498.831371028\n",
      "Loss: 848535.3477626815\n",
      "Loss: 840506.2440430885\n",
      "Loss: 758963.2607607245\n",
      "Loss: 753872.9918505447\n",
      "Loss: 781155.3208053526\n",
      "Loss: 721808.0232726799\n",
      "Epoch: 000, Train Acc: 0.9036, Test Acc: 0.9036\n",
      "Loss: 878038.3788245593\n",
      "Loss: 638357.3455188944\n",
      "Loss: 806388.2899493701\n",
      "Loss: 621185.9055017235\n",
      "Loss: 706129.8888481255\n",
      "Loss: 643030.0387704755\n",
      "Loss: 651210.1445196174\n",
      "Loss: 592200.0610192926\n",
      "Loss: 852855.602882339\n",
      "Loss: 786939.9729419276\n",
      "Loss: 733596.2669292369\n",
      "Loss: 654977.0701860627\n",
      "Loss: 560120.3401853865\n",
      "Loss: 524123.13036753016\n",
      "Loss: 585457.550631804\n",
      "Loss: 527839.1274052319\n",
      "Loss: 561034.968339666\n",
      "Loss: 527693.8499376727\n",
      "Loss: 491957.9769355856\n",
      "Loss: 510561.1387867653\n",
      "Loss: 490680.44225988706\n",
      "Loss: 392371.4839613924\n",
      "Loss: 405399.3024247512\n",
      "Loss: 409080.89660026395\n",
      "Loss: 355345.1275833001\n",
      "Loss: 332811.0911978356\n",
      "Loss: 306685.04696848517\n",
      "Loss: 357848.37457487604\n",
      "Loss: 300792.05130618415\n",
      "Loss: 319122.4588406286\n",
      "Loss: 279510.52347292524\n",
      "Loss: 253527.08761071722\n",
      "Loss: 212171.1830978589\n",
      "Loss: 190461.89521892826\n",
      "Loss: 181826.61400404258\n",
      "Loss: 204876.52515627007\n",
      "Loss: 167565.61228128185\n",
      "Loss: 159611.54335554352\n",
      "Loss: 143880.316169947\n",
      "Loss: 136561.3311445806\n",
      "Loss: 142696.29801128118\n",
      "Loss: 138051.32891449647\n",
      "Loss: 130979.98208366033\n",
      "Loss: 125069.7717055575\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\lokix\\OneDrive\\Documents\\univsersidad\\MASTER\\1B\\TFM\\src\\GNNCopyModel\\trainEEGGNN.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 59>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lokix/OneDrive/Documents/univsersidad/MASTER/1B/TFM/src/GNNCopyModel/trainEEGGNN.ipynb#ch0000005?line=59'>60</a>\u001b[0m train()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lokix/OneDrive/Documents/univsersidad/MASTER/1B/TFM/src/GNNCopyModel/trainEEGGNN.ipynb#ch0000005?line=60'>61</a>\u001b[0m train_acc \u001b[39m=\u001b[39m test(train_batches)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/lokix/OneDrive/Documents/univsersidad/MASTER/1B/TFM/src/GNNCopyModel/trainEEGGNN.ipynb#ch0000005?line=61'>62</a>\u001b[0m test_acc \u001b[39m=\u001b[39m test(test_batches)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lokix/OneDrive/Documents/univsersidad/MASTER/1B/TFM/src/GNNCopyModel/trainEEGGNN.ipynb#ch0000005?line=62'>63</a>\u001b[0m \u001b[39mprint\u001b[39m(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lokix/OneDrive/Documents/univsersidad/MASTER/1B/TFM/src/GNNCopyModel/trainEEGGNN.ipynb#ch0000005?line=63'>64</a>\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m:\u001b[39;00m\u001b[39m03d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Train Acc: \u001b[39m\u001b[39m{\u001b[39;00mtrain_acc\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Test Acc: \u001b[39m\u001b[39m{\u001b[39;00mtest_acc\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lokix/OneDrive/Documents/univsersidad/MASTER/1B/TFM/src/GNNCopyModel/trainEEGGNN.ipynb#ch0000005?line=64'>65</a>\u001b[0m )\n",
      "\u001b[1;32mc:\\Users\\lokix\\OneDrive\\Documents\\univsersidad\\MASTER\\1B\\TFM\\src\\GNNCopyModel\\trainEEGGNN.ipynb Cell 8'\u001b[0m in \u001b[0;36mtest\u001b[1;34m(loader)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lokix/OneDrive/Documents/univsersidad/MASTER/1B/TFM/src/GNNCopyModel/trainEEGGNN.ipynb#ch0000005?line=38'>39</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lokix/OneDrive/Documents/univsersidad/MASTER/1B/TFM/src/GNNCopyModel/trainEEGGNN.ipynb#ch0000005?line=40'>41</a>\u001b[0m correct \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/lokix/OneDrive/Documents/univsersidad/MASTER/1B/TFM/src/GNNCopyModel/trainEEGGNN.ipynb#ch0000005?line=41'>42</a>\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m loader:  \u001b[39m# Iterate in batches over the training/test dataset.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lokix/OneDrive/Documents/univsersidad/MASTER/1B/TFM/src/GNNCopyModel/trainEEGGNN.ipynb#ch0000005?line=42'>43</a>\u001b[0m     out \u001b[39m=\u001b[39m model(data\u001b[39m.\u001b[39mx, data\u001b[39m.\u001b[39medge_index, data\u001b[39m.\u001b[39medge_weight, data\u001b[39m.\u001b[39mbatch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lokix/OneDrive/Documents/univsersidad/MASTER/1B/TFM/src/GNNCopyModel/trainEEGGNN.ipynb#ch0000005?line=43'>44</a>\u001b[0m     pred \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)  \u001b[39m# Use the class with highest probability.\u001b[39;00m\n",
      "File \u001b[1;32m~\\Envs\\tfm\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/lokix/Envs/tfm/lib/site-packages/torch/utils/data/dataloader.py?line=527'>528</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/lokix/Envs/tfm/lib/site-packages/torch/utils/data/dataloader.py?line=528'>529</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> <a href='file:///c%3A/Users/lokix/Envs/tfm/lib/site-packages/torch/utils/data/dataloader.py?line=529'>530</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    <a href='file:///c%3A/Users/lokix/Envs/tfm/lib/site-packages/torch/utils/data/dataloader.py?line=530'>531</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/lokix/Envs/tfm/lib/site-packages/torch/utils/data/dataloader.py?line=531'>532</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    <a href='file:///c%3A/Users/lokix/Envs/tfm/lib/site-packages/torch/utils/data/dataloader.py?line=532'>533</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    <a href='file:///c%3A/Users/lokix/Envs/tfm/lib/site-packages/torch/utils/data/dataloader.py?line=533'>534</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\Envs\\tfm\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/lokix/Envs/tfm/lib/site-packages/torch/utils/data/dataloader.py?line=567'>568</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/lokix/Envs/tfm/lib/site-packages/torch/utils/data/dataloader.py?line=568'>569</a>\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/lokix/Envs/tfm/lib/site-packages/torch/utils/data/dataloader.py?line=569'>570</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/lokix/Envs/tfm/lib/site-packages/torch/utils/data/dataloader.py?line=570'>571</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    <a href='file:///c%3A/Users/lokix/Envs/tfm/lib/site-packages/torch/utils/data/dataloader.py?line=571'>572</a>\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32m~\\Envs\\tfm\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/lokix/Envs/tfm/lib/site-packages/torch/utils/data/_utils/fetch.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     <a href='file:///c%3A/Users/lokix/Envs/tfm/lib/site-packages/torch/utils/data/_utils/fetch.py?line=47'>48</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> <a href='file:///c%3A/Users/lokix/Envs/tfm/lib/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     <a href='file:///c%3A/Users/lokix/Envs/tfm/lib/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/lokix/Envs/tfm/lib/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\Envs\\tfm\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/lokix/Envs/tfm/lib/site-packages/torch/utils/data/_utils/fetch.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     <a href='file:///c%3A/Users/lokix/Envs/tfm/lib/site-packages/torch/utils/data/_utils/fetch.py?line=47'>48</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> <a href='file:///c%3A/Users/lokix/Envs/tfm/lib/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     <a href='file:///c%3A/Users/lokix/Envs/tfm/lib/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/lokix/Envs/tfm/lib/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\lokix\\OneDrive\\Documents\\univsersidad\\MASTER\\1B\\TFM\\src\\GNNCopyModel\\EEGDataset.py:230\u001b[0m, in \u001b[0;36mEEGGraphDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/lokix/OneDrive/Documents/univsersidad/MASTER/1B/TFM/src/GNNCopyModel/EEGDataset.py?line=227'>228</a>\u001b[0m \u001b[39m# combine edge weights and spect coh values into one value/ one E x 1 tensor\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/lokix/OneDrive/Documents/univsersidad/MASTER/1B/TFM/src/GNNCopyModel/EEGDataset.py?line=228'>229</a>\u001b[0m edge_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistances \u001b[39m+\u001b[39m spec_coh_values\n\u001b[1;32m--> <a href='file:///c%3A/Users/lokix/OneDrive/Documents/univsersidad/MASTER/1B/TFM/src/GNNCopyModel/EEGDataset.py?line=229'>230</a>\u001b[0m edge_weights \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(edge_weights)\n\u001b[0;32m    <a href='file:///c%3A/Users/lokix/OneDrive/Documents/univsersidad/MASTER/1B/TFM/src/GNNCopyModel/EEGDataset.py?line=231'>232</a>\u001b[0m \u001b[39m# NOTE: taken from https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html#\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/lokix/OneDrive/Documents/univsersidad/MASTER/1B/TFM/src/GNNCopyModel/EEGDataset.py?line=232'>233</a>\u001b[0m \u001b[39m# https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/lokix/OneDrive/Documents/univsersidad/MASTER/1B/TFM/src/GNNCopyModel/EEGDataset.py?line=233'>234</a>\u001b[0m data \u001b[39m=\u001b[39m Data(x\u001b[39m=\u001b[39mnode_features, \n\u001b[0;32m    <a href='file:///c%3A/Users/lokix/OneDrive/Documents/univsersidad/MASTER/1B/TFM/src/GNNCopyModel/EEGDataset.py?line=234'>235</a>\u001b[0m \t\t\tedge_index\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39medge_index, \n\u001b[0;32m    <a href='file:///c%3A/Users/lokix/OneDrive/Documents/univsersidad/MASTER/1B/TFM/src/GNNCopyModel/EEGDataset.py?line=235'>236</a>\u001b[0m \t\t\tedge_attr\u001b[39m=\u001b[39medge_weights,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/lokix/OneDrive/Documents/univsersidad/MASTER/1B/TFM/src/GNNCopyModel/EEGDataset.py?line=238'>239</a>\u001b[0m \t\t\t\u001b[39m# pos=None, norm=None, face=None, **kwargs\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/lokix/OneDrive/Documents/univsersidad/MASTER/1B/TFM/src/GNNCopyModel/EEGDataset.py?line=239'>240</a>\u001b[0m \t\t\t)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = EEGGNN(True, SFREQ, BATCH_SIZE)\n",
    "model = model.double()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=torch.Tensor([disease_count, healthy_count]))\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    i = 0\n",
    "    loss_value = 0\n",
    "    for data in train_batches:  # Iterate in batches over the training dataset.\n",
    "        i += 1\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "        data.batch = data.batch.view(data.batch.shape[0], -1)\n",
    "        out = model(data.x, data.edge_index, data.edge_weight,\n",
    "                    data.batch)  # Perform a single forward pass.\n",
    "        #print(out)\n",
    "        #print(out.shape)\n",
    "        #print(data.y)\n",
    "        #print(data.y.shape)\n",
    "        \n",
    "        \"\"\"print(\"Output: \")\n",
    "        print(out)\n",
    "        print(\"Ground truth\")\n",
    "        print(data.y)\"\"\"\n",
    "        loss = criterion(out, data.y)  # Compute the loss\n",
    "        loss_value += loss.item()\n",
    "        \n",
    "        if i%10 == 0:\n",
    "            print(f\"Loss: {(loss_value) / 100}\")\n",
    "            loss_value = 0\n",
    "            \n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        \n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "        out = model(data.x, data.edge_index, data.edge_weight, data.batch)\n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        \"\"\"print(\"Output: \")\n",
    "        print(out)\n",
    "        print(\"Predictions: \")\n",
    "        print(pred)\n",
    "        print(\"Labels: \")\n",
    "        print(data.y)\n",
    "        print(\"Argmax: \")\n",
    "        print(data.y.argmax(dim=1))\"\"\"\n",
    "        correct += int(\n",
    "            (pred == data.y.argmax(dim=1)).sum())  # Check against ground-truth labels.\n",
    "    return correct / len(\n",
    "        loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "\n",
    "for epoch in range(200):\n",
    "    train()\n",
    "    train_acc = test(train_batches)\n",
    "    test_acc = test(test_batches)\n",
    "    print(\n",
    "        f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "49828187f4dc564a08cffdc3b9732e1c7952dfdfaa36f705d81755fe28017e2e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 ('tfm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
